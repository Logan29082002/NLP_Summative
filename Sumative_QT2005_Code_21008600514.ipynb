{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\logan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\logan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\logan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PRAW in c:\\users\\logan\\anaconda3\\lib\\site-packages (7.6.1)\n",
      "Requirement already satisfied: prawcore<3,>=2.1 in c:\\users\\logan\\anaconda3\\lib\\site-packages (from PRAW) (2.3.0)\n",
      "Requirement already satisfied: update-checker>=0.18 in c:\\users\\logan\\anaconda3\\lib\\site-packages (from PRAW) (0.18.0)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in c:\\users\\logan\\anaconda3\\lib\\site-packages (from PRAW) (1.5.1)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in c:\\users\\logan\\anaconda3\\lib\\site-packages (from prawcore<3,>=2.1->PRAW) (2.24.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\logan\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->PRAW) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\logan\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->PRAW) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\logan\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->PRAW) (1.25.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\logan\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->PRAW) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "#import all relvant libraries\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from nltk import stem\n",
    "stemmer = stem.PorterStemmer()\n",
    "from nltk import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words('english'))\n",
    "import string\n",
    "punct = list(string.punctuation)\n",
    "from collections import Counter\n",
    "import requests\n",
    "import pandas as pd\n",
    "!pip install PRAW\n",
    "import numpy as np\n",
    "import praw\n",
    "import datetime\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set request credentials to a variable which can be called upon \n",
    "\n",
    "reddit = praw.Reddit(user_agent='VAD',\n",
    "                     client_id='3MelI32fEig1elS4ztSdkA', client_secret=\"mqWydYBCGDizuaBVIqeGfrJ8X7pr-A\",\n",
    "                     username='xxxxxxxxx', password='xxxxxxxxxx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kmiller0112\n",
      "PuckElectra\n",
      "all_about_everyone\n",
      "PuzzleheadedYam5996\n",
      "TextDetectorBot\n",
      "streets-a-head\n",
      "feelinn\n",
      "Icarusmacsj1149\n",
      "boringname101\n",
      "jayfelee139\n"
     ]
    }
   ],
   "source": [
    "#checking all good. Test code to see if all the submission requests ect work\n",
    "\n",
    "for submission in reddit.subreddit(\"test\").hot(limit=10):\n",
    "    print(submission.author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submission(submission_id): ## submission_id can be URL or submission ID\n",
    "    try:\n",
    "        submission = reddit.submission(url = submission_id)\n",
    "    except:\n",
    "        submission = reddit.submission(submission_id)\n",
    "        \n",
    "    #collecting and presenting\n",
    "    title = submission.title\n",
    "    submission.comments.replace_more() ## loads new page if cooments are multipage\n",
    "    text = [i.body for i in submission.comments]\n",
    "    score = [i.score for i in submission.comments]\n",
    "    user = [i.author for i in submission.comments]\n",
    "    date = [datetime.datetime.fromtimestamp(i.created) for i in submission.comments]\n",
    "    df = pd.DataFrame()\n",
    "    df['text'] = text\n",
    "    df['datetime'] = date\n",
    "    df['score'] = score \n",
    "    df['subreddit'] = submission.subreddit\n",
    "    df['redditor'] = user\n",
    "    df['type'] = 'comment'\n",
    "    df['title'] = title\n",
    "    \n",
    "    #prepping the data\n",
    "    df['redditor_hashed'] = df['redditor'].apply(lambda x: hashlib.sha256(str(x).encode()).hexdigest()) ##anonymises the username \n",
    "    df['text_tokenised']= df['text'].apply(word_tokenize) #tokenises the comment\n",
    "    df['text_lowered']= df['text_tokenised'].apply(lambda tok_list: [i.lower() for i in tok_list]) #puts it into lower case\n",
    "    df['text_without_stopwords'] = [[word for word in row if word not in stops and word not in punct] for row in df['text_lowered']] #removes stopwords\n",
    "    df['text_clean'] = [[word.encode('ascii', 'ignore').decode() for word in row if word not in stops and word not in punct] for row in df['text_lowered']] #converts everything to ASCII and ignores all characters that are unconvertable \n",
    "    df['text_lemmatised']= df['text_lowered'].apply(lambda lem_list: [lemmatizer.lemmatize(i) for i in lem_list]) #lemmatizes remaining data\n",
    "    df = df.drop('redditor', axis=1)\n",
    "    \n",
    "    df = df.sort_values('score', ascending = False).reset_index(drop = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing the request and preparation of the data using the submission() function \n",
    "df = submission('https://www.reddit.com/r/Damnthatsinteresting/comments/jmln03/in_south_korea_the_solar_panels_in_the_middle_of/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>datetime</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>type</th>\n",
       "      <th>title</th>\n",
       "      <th>redditor_hashed</th>\n",
       "      <th>text_tokenised</th>\n",
       "      <th>text_lowered</th>\n",
       "      <th>text_without_stopwords</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_lemmatised</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This idea is great. Unfortunately, I stay in F...</td>\n",
       "      <td>2020-11-02 12:01:56</td>\n",
       "      <td>5315</td>\n",
       "      <td>Damnthatsinteresting</td>\n",
       "      <td>comment</td>\n",
       "      <td>In South Korea, the solar panels in the middle...</td>\n",
       "      <td>dc937b59892604f5a86ac96936cd7ff09e25f18ae6b758...</td>\n",
       "      <td>[This, idea, is, great, ., Unfortunately, ,, I...</td>\n",
       "      <td>[this, idea, is, great, ., unfortunately, ,, i...</td>\n",
       "      <td>[idea, great, unfortunately, stay, florida, fe...</td>\n",
       "      <td>[idea, great, unfortunately, stay, florida, fe...</td>\n",
       "      <td>[this, idea, is, great, ., unfortunately, ,, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm from SK but I've never seen this in my lif...</td>\n",
       "      <td>2020-11-02 13:02:00</td>\n",
       "      <td>1802</td>\n",
       "      <td>Damnthatsinteresting</td>\n",
       "      <td>comment</td>\n",
       "      <td>In South Korea, the solar panels in the middle...</td>\n",
       "      <td>9f857e1a8855405dee584f3b0a2fac14c909642406de01...</td>\n",
       "      <td>[I, 'm, from, SK, but, I, 've, never, seen, th...</td>\n",
       "      <td>[i, 'm, from, sk, but, i, 've, never, seen, th...</td>\n",
       "      <td>['m, sk, 've, never, seen, life, googling, app...</td>\n",
       "      <td>['m, sk, 've, never, seen, life, googling, app...</td>\n",
       "      <td>[i, 'm, from, sk, but, i, 've, never, seen, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Okey.. nice. But how do you get on/off the bik...</td>\n",
       "      <td>2020-11-02 12:21:54</td>\n",
       "      <td>1387</td>\n",
       "      <td>Damnthatsinteresting</td>\n",
       "      <td>comment</td>\n",
       "      <td>In South Korea, the solar panels in the middle...</td>\n",
       "      <td>a21e3373140cb2d76eb06608a684a14ee1ff8cbf522943...</td>\n",
       "      <td>[Okey, .., nice, ., But, how, do, you, get, on...</td>\n",
       "      <td>[okey, .., nice, ., but, how, do, you, get, on...</td>\n",
       "      <td>[okey, .., nice, get, on/off, bike, path]</td>\n",
       "      <td>[okey, .., nice, get, on/off, bike, path]</td>\n",
       "      <td>[okey, .., nice, ., but, how, do, you, get, on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Americans: This is an amazing idea. \\nDutch Gu...</td>\n",
       "      <td>2020-11-02 12:26:04</td>\n",
       "      <td>486</td>\n",
       "      <td>Damnthatsinteresting</td>\n",
       "      <td>comment</td>\n",
       "      <td>In South Korea, the solar panels in the middle...</td>\n",
       "      <td>859600e9cf22435d4a71fa55fa336be183f38147cb013c...</td>\n",
       "      <td>[Americans, :, This, is, an, amazing, idea, .,...</td>\n",
       "      <td>[americans, :, this, is, an, amazing, idea, .,...</td>\n",
       "      <td>[americans, amazing, idea, dutch, guy, exhaust...</td>\n",
       "      <td>[americans, amazing, idea, dutch, guy, exhaust...</td>\n",
       "      <td>[american, :, this, is, an, amazing, idea, ., ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We can't have that here. America is special in...</td>\n",
       "      <td>2020-11-02 11:37:46</td>\n",
       "      <td>366</td>\n",
       "      <td>Damnthatsinteresting</td>\n",
       "      <td>comment</td>\n",
       "      <td>In South Korea, the solar panels in the middle...</td>\n",
       "      <td>490294b97dd9c899da4a4e5ba8331583b943d2e3d45e3a...</td>\n",
       "      <td>[We, ca, n't, have, that, here, ., America, is...</td>\n",
       "      <td>[we, ca, n't, have, that, here, ., america, is...</td>\n",
       "      <td>[ca, n't, america, special, every, special, ci...</td>\n",
       "      <td>[ca, n't, america, special, every, special, ci...</td>\n",
       "      <td>[we, ca, n't, have, that, here, ., america, is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>Miles ahead of the rest of the world</td>\n",
       "      <td>2020-11-02 12:30:24</td>\n",
       "      <td>-3</td>\n",
       "      <td>Damnthatsinteresting</td>\n",
       "      <td>comment</td>\n",
       "      <td>In South Korea, the solar panels in the middle...</td>\n",
       "      <td>5a4c719dda49fb80c8395c1ed7182677c12b2955869cbe...</td>\n",
       "      <td>[Miles, ahead, of, the, rest, of, the, world]</td>\n",
       "      <td>[miles, ahead, of, the, rest, of, the, world]</td>\n",
       "      <td>[miles, ahead, rest, world]</td>\n",
       "      <td>[miles, ahead, rest, world]</td>\n",
       "      <td>[mile, ahead, of, the, rest, of, the, world]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>903</th>\n",
       "      <td>Comment section is full of Americans, again. R...</td>\n",
       "      <td>2020-11-02 13:35:14</td>\n",
       "      <td>-4</td>\n",
       "      <td>Damnthatsinteresting</td>\n",
       "      <td>comment</td>\n",
       "      <td>In South Korea, the solar panels in the middle...</td>\n",
       "      <td>c201f7273bd55b72de573f8295c503a86259ad462427b2...</td>\n",
       "      <td>[Comment, section, is, full, of, Americans, ,,...</td>\n",
       "      <td>[comment, section, is, full, of, americans, ,,...</td>\n",
       "      <td>[comment, section, full, americans, reddit, ne...</td>\n",
       "      <td>[comment, section, full, americans, reddit, ne...</td>\n",
       "      <td>[comment, section, is, full, of, american, ,, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>Its like totally logical.  Will never happen i...</td>\n",
       "      <td>2020-11-02 12:13:22</td>\n",
       "      <td>-5</td>\n",
       "      <td>Damnthatsinteresting</td>\n",
       "      <td>comment</td>\n",
       "      <td>In South Korea, the solar panels in the middle...</td>\n",
       "      <td>dc937b59892604f5a86ac96936cd7ff09e25f18ae6b758...</td>\n",
       "      <td>[Its, like, totally, logical, ., Will, never, ...</td>\n",
       "      <td>[its, like, totally, logical, ., will, never, ...</td>\n",
       "      <td>[like, totally, logical, never, happen, merica]</td>\n",
       "      <td>[like, totally, logical, never, happen, merica]</td>\n",
       "      <td>[it, like, totally, logical, ., will, never, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>[removed]</td>\n",
       "      <td>2020-11-02 13:27:23</td>\n",
       "      <td>-6</td>\n",
       "      <td>Damnthatsinteresting</td>\n",
       "      <td>comment</td>\n",
       "      <td>In South Korea, the solar panels in the middle...</td>\n",
       "      <td>dc937b59892604f5a86ac96936cd7ff09e25f18ae6b758...</td>\n",
       "      <td>[[, removed, ]]</td>\n",
       "      <td>[[, removed, ]]</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>[[, removed, ]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>While they eat hotdogs made from actual dogs...</td>\n",
       "      <td>2020-11-02 14:38:33</td>\n",
       "      <td>-7</td>\n",
       "      <td>Damnthatsinteresting</td>\n",
       "      <td>comment</td>\n",
       "      <td>In South Korea, the solar panels in the middle...</td>\n",
       "      <td>a02c22adeee08ab5baa041484f38dbb6f0686b72f803da...</td>\n",
       "      <td>[While, they, eat, hotdogs, made, from, actual...</td>\n",
       "      <td>[while, they, eat, hotdogs, made, from, actual...</td>\n",
       "      <td>[eat, hotdogs, made, actual, dogs, ...]</td>\n",
       "      <td>[eat, hotdogs, made, actual, dogs, ...]</td>\n",
       "      <td>[while, they, eat, hotdog, made, from, actual,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>907 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text            datetime  \\\n",
       "0    This idea is great. Unfortunately, I stay in F... 2020-11-02 12:01:56   \n",
       "1    I'm from SK but I've never seen this in my lif... 2020-11-02 13:02:00   \n",
       "2    Okey.. nice. But how do you get on/off the bik... 2020-11-02 12:21:54   \n",
       "3    Americans: This is an amazing idea. \\nDutch Gu... 2020-11-02 12:26:04   \n",
       "4    We can't have that here. America is special in... 2020-11-02 11:37:46   \n",
       "..                                                 ...                 ...   \n",
       "902               Miles ahead of the rest of the world 2020-11-02 12:30:24   \n",
       "903  Comment section is full of Americans, again. R... 2020-11-02 13:35:14   \n",
       "904  Its like totally logical.  Will never happen i... 2020-11-02 12:13:22   \n",
       "905                                          [removed] 2020-11-02 13:27:23   \n",
       "906    While they eat hotdogs made from actual dogs... 2020-11-02 14:38:33   \n",
       "\n",
       "     score             subreddit     type  \\\n",
       "0     5315  Damnthatsinteresting  comment   \n",
       "1     1802  Damnthatsinteresting  comment   \n",
       "2     1387  Damnthatsinteresting  comment   \n",
       "3      486  Damnthatsinteresting  comment   \n",
       "4      366  Damnthatsinteresting  comment   \n",
       "..     ...                   ...      ...   \n",
       "902     -3  Damnthatsinteresting  comment   \n",
       "903     -4  Damnthatsinteresting  comment   \n",
       "904     -5  Damnthatsinteresting  comment   \n",
       "905     -6  Damnthatsinteresting  comment   \n",
       "906     -7  Damnthatsinteresting  comment   \n",
       "\n",
       "                                                 title  \\\n",
       "0    In South Korea, the solar panels in the middle...   \n",
       "1    In South Korea, the solar panels in the middle...   \n",
       "2    In South Korea, the solar panels in the middle...   \n",
       "3    In South Korea, the solar panels in the middle...   \n",
       "4    In South Korea, the solar panels in the middle...   \n",
       "..                                                 ...   \n",
       "902  In South Korea, the solar panels in the middle...   \n",
       "903  In South Korea, the solar panels in the middle...   \n",
       "904  In South Korea, the solar panels in the middle...   \n",
       "905  In South Korea, the solar panels in the middle...   \n",
       "906  In South Korea, the solar panels in the middle...   \n",
       "\n",
       "                                       redditor_hashed  \\\n",
       "0    dc937b59892604f5a86ac96936cd7ff09e25f18ae6b758...   \n",
       "1    9f857e1a8855405dee584f3b0a2fac14c909642406de01...   \n",
       "2    a21e3373140cb2d76eb06608a684a14ee1ff8cbf522943...   \n",
       "3    859600e9cf22435d4a71fa55fa336be183f38147cb013c...   \n",
       "4    490294b97dd9c899da4a4e5ba8331583b943d2e3d45e3a...   \n",
       "..                                                 ...   \n",
       "902  5a4c719dda49fb80c8395c1ed7182677c12b2955869cbe...   \n",
       "903  c201f7273bd55b72de573f8295c503a86259ad462427b2...   \n",
       "904  dc937b59892604f5a86ac96936cd7ff09e25f18ae6b758...   \n",
       "905  dc937b59892604f5a86ac96936cd7ff09e25f18ae6b758...   \n",
       "906  a02c22adeee08ab5baa041484f38dbb6f0686b72f803da...   \n",
       "\n",
       "                                        text_tokenised  \\\n",
       "0    [This, idea, is, great, ., Unfortunately, ,, I...   \n",
       "1    [I, 'm, from, SK, but, I, 've, never, seen, th...   \n",
       "2    [Okey, .., nice, ., But, how, do, you, get, on...   \n",
       "3    [Americans, :, This, is, an, amazing, idea, .,...   \n",
       "4    [We, ca, n't, have, that, here, ., America, is...   \n",
       "..                                                 ...   \n",
       "902      [Miles, ahead, of, the, rest, of, the, world]   \n",
       "903  [Comment, section, is, full, of, Americans, ,,...   \n",
       "904  [Its, like, totally, logical, ., Will, never, ...   \n",
       "905                                    [[, removed, ]]   \n",
       "906  [While, they, eat, hotdogs, made, from, actual...   \n",
       "\n",
       "                                          text_lowered  \\\n",
       "0    [this, idea, is, great, ., unfortunately, ,, i...   \n",
       "1    [i, 'm, from, sk, but, i, 've, never, seen, th...   \n",
       "2    [okey, .., nice, ., but, how, do, you, get, on...   \n",
       "3    [americans, :, this, is, an, amazing, idea, .,...   \n",
       "4    [we, ca, n't, have, that, here, ., america, is...   \n",
       "..                                                 ...   \n",
       "902      [miles, ahead, of, the, rest, of, the, world]   \n",
       "903  [comment, section, is, full, of, americans, ,,...   \n",
       "904  [its, like, totally, logical, ., will, never, ...   \n",
       "905                                    [[, removed, ]]   \n",
       "906  [while, they, eat, hotdogs, made, from, actual...   \n",
       "\n",
       "                                text_without_stopwords  \\\n",
       "0    [idea, great, unfortunately, stay, florida, fe...   \n",
       "1    ['m, sk, 've, never, seen, life, googling, app...   \n",
       "2            [okey, .., nice, get, on/off, bike, path]   \n",
       "3    [americans, amazing, idea, dutch, guy, exhaust...   \n",
       "4    [ca, n't, america, special, every, special, ci...   \n",
       "..                                                 ...   \n",
       "902                        [miles, ahead, rest, world]   \n",
       "903  [comment, section, full, americans, reddit, ne...   \n",
       "904    [like, totally, logical, never, happen, merica]   \n",
       "905                                          [removed]   \n",
       "906            [eat, hotdogs, made, actual, dogs, ...]   \n",
       "\n",
       "                                            text_clean  \\\n",
       "0    [idea, great, unfortunately, stay, florida, fe...   \n",
       "1    ['m, sk, 've, never, seen, life, googling, app...   \n",
       "2            [okey, .., nice, get, on/off, bike, path]   \n",
       "3    [americans, amazing, idea, dutch, guy, exhaust...   \n",
       "4    [ca, n't, america, special, every, special, ci...   \n",
       "..                                                 ...   \n",
       "902                        [miles, ahead, rest, world]   \n",
       "903  [comment, section, full, americans, reddit, ne...   \n",
       "904    [like, totally, logical, never, happen, merica]   \n",
       "905                                          [removed]   \n",
       "906            [eat, hotdogs, made, actual, dogs, ...]   \n",
       "\n",
       "                                       text_lemmatised  \n",
       "0    [this, idea, is, great, ., unfortunately, ,, i...  \n",
       "1    [i, 'm, from, sk, but, i, 've, never, seen, th...  \n",
       "2    [okey, .., nice, ., but, how, do, you, get, on...  \n",
       "3    [american, :, this, is, an, amazing, idea, ., ...  \n",
       "4    [we, ca, n't, have, that, here, ., america, is...  \n",
       "..                                                 ...  \n",
       "902       [mile, ahead, of, the, rest, of, the, world]  \n",
       "903  [comment, section, is, full, of, american, ,, ...  \n",
       "904  [it, like, totally, logical, ., will, never, h...  \n",
       "905                                    [[, removed, ]]  \n",
       "906  [while, they, eat, hotdog, made, from, actual,...  \n",
       "\n",
       "[907 rows x 12 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add the URLs of the posts I want to extract comments from, to the list of the theme it relates to, so that when I itterate through, the  comments are seperated by post and aslo the request isn't as big  \n",
    "\n",
    "solar_innovations = ['https://www.reddit.com/r/Damnthatsinteresting/comments/jmln03/in_south_korea_the_solar_panels_in_the_middle_of/' ,\n",
    "                     'https://www.reddit.com/r/nextfuckinglevel/comments/ipei37/transparent_solar_panels_will_turn_windows_into/',\n",
    "                     'https://www.reddit.com/r/technology/comments/fwlgim/oil_companies_are_collapsing_but_wind_and_solar/',\n",
    "                     'https://www.reddit.com/r/science/comments/8h6uot/the_sea_slug_elysia_chlorotica_is_able_to_become/',\n",
    "                     'https://www.reddit.com/r/science/comments/hgccyn/scientists_identify_a_novel_method_to_create/',\n",
    "                     'https://www.reddit.com/r/Futurology/comments/7e5ir1/solar_power_is_now_the_cheapest_form_of_energy_on/',\n",
    "                     'https://www.reddit.com/r/UpliftingNews/comments/10dc0iu/the_price_of_solar_panels_is_set_to_plunge/',\n",
    "                     'https://www.reddit.com/r/energy/comments/114pzga/china_now_has_enough_wind_and_solar_to_power/']\n",
    "                                 \n",
    "Wind_power = ['https://www.reddit.com/r/worldnews/comments/ceejtt/scotland_is_now_generating_so_much_wind_energy_it/',\n",
    "            'https://www.reddit.com/r/worldnews/comments/hze0sx/climate_crisis_offshore_wind_power_so_cheap_it/',\n",
    "            'https://www.reddit.com/r/technology/comments/7ab7l1/there_was_so_much_wind_power_in_germany_this/',\n",
    "            'https://www.reddit.com/r/worldnews/comments/ld5ebz/south_korea_unveils_43_billion_plan_for_worlds/',\n",
    "            'https://www.reddit.com/r/news/comments/mfu8de/the_biden_administration_makes_a_swath_of_ocean/',\n",
    "            'https://www.reddit.com/r/europe/comments/usxnn0/denmark_netherlands_germany_and_belgium_sign_135/',\n",
    "            'https://www.reddit.com/r/Futurology/comments/xoj7sb/offshore_wind_125_times_better_for_taxpayers/']\n",
    "    \n",
    "Nuclear_Fission = ['https://www.reddit.com/r/unpopularopinion/comments/jj7y3h/if_you_actually_care_about_the_environment/',\n",
    "                'https://www.reddit.com/r/science/comments/20b7v9/science_ama_series_were_professors_in_the/']\n",
    "    \n",
    "Nuclear_Fusion = ['https://www.reddit.com/r/technology/comments/zjfgxb/us_scientists_achieve_holy_grail_net_gain_nuclear/',\n",
    "                'https://www.reddit.com/r/worldnews/comments/hzgwg4/fourteen_years_after_receiving_the_official/',\n",
    "                'https://www.reddit.com/r/videos/comments/zl7vlx/nuclear_scientist_marv_adams_explains_what/',\n",
    "                'https://www.reddit.com/r/technology/comments/wmwreq/nuclear_fusion_breakthrough_confirmed_california/',\n",
    "                'https://www.reddit.com/r/Futurology/comments/wmj0kr/nuclear_fusion_ignition_confirmed_in_an/',\n",
    "                'https://www.reddit.com/r/technology/comments/5grbo2/tests_confirm_that_germanys_massive_nuclear/']\n",
    "                \n",
    "Hydro_power = ['https://www.reddit.com/r/Damnthatsinteresting/comments/wqub34/the_belgian_startup_turbulent_hydro_has_invented/']\n",
    "               \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new DataFrame to which I can append the data structues as they go through the iteration\n",
    "\n",
    "df_combined = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i have been added\n",
      "i have been added\n",
      "i have been added\n",
      "i have been added\n",
      "i have been added\n",
      "i have been added\n",
      "i have been added\n",
      "i have been added\n"
     ]
    }
   ],
   "source": [
    "#For loop to iterate through the links in the list and the print function is added so that I know it's working \n",
    "#The same code is used in the next few cells for each list\n",
    "\n",
    "for url in solar_innovations:\n",
    "    df = submission(url)\n",
    "    df_combined_1 = pd.concat([df_combined,df])\n",
    "    print('i have been added')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i have been added\n",
      "i have been added\n",
      "i have been added\n",
      "i have been added\n",
      "i have been added\n",
      "i have been added\n",
      "i have been added\n"
     ]
    }
   ],
   "source": [
    "for url in Wind_power:\n",
    "    df = submission(url)\n",
    "    df_combined = pd.concat([df_combined,df])\n",
    "    print('i have been added')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i have been added\n",
      "i have been added\n"
     ]
    }
   ],
   "source": [
    "for url in Nuclear_Fission:\n",
    "    df = submission(url)\n",
    "    df_combined = pd.concat([df_combined,df])\n",
    "    print('i have been added')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i have been added\n",
      "i have been added\n",
      "i have been added\n",
      "i have been added\n",
      "i have been added\n",
      "i have been added\n"
     ]
    }
   ],
   "source": [
    "for url in Nuclear_Fusion:\n",
    "    df = submission(url)\n",
    "    df_combined = pd.concat([df_combined,df])\n",
    "    print('i have been added')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i have been added\n"
     ]
    }
   ],
   "source": [
    "for url in Hydro_power:\n",
    "    df = submission(url)\n",
    "    df_combined = pd.concat([df_combined,df])\n",
    "    print('i have been added')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>datetime</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>type</th>\n",
       "      <th>title</th>\n",
       "      <th>redditor_hashed</th>\n",
       "      <th>text_tokenised</th>\n",
       "      <th>text_lowered</th>\n",
       "      <th>text_without_stopwords</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_lemmatised</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This idea is great. Unfortunately, I stay in F...</td>\n",
       "      <td>2020-11-02 12:01:56</td>\n",
       "      <td>5318</td>\n",
       "      <td>Damnthatsinteresting</td>\n",
       "      <td>comment</td>\n",
       "      <td>In South Korea, the solar panels in the middle...</td>\n",
       "      <td>dc937b59892604f5a86ac96936cd7ff09e25f18ae6b758...</td>\n",
       "      <td>[This, idea, is, great, ., Unfortunately, ,, I...</td>\n",
       "      <td>[this, idea, is, great, ., unfortunately, ,, i...</td>\n",
       "      <td>[idea, great, unfortunately, stay, florida, fe...</td>\n",
       "      <td>[idea, great, unfortunately, stay, florida, fe...</td>\n",
       "      <td>[this, idea, is, great, ., unfortunately, ,, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm from SK but I've never seen this in my lif...</td>\n",
       "      <td>2020-11-02 13:02:00</td>\n",
       "      <td>1798</td>\n",
       "      <td>Damnthatsinteresting</td>\n",
       "      <td>comment</td>\n",
       "      <td>In South Korea, the solar panels in the middle...</td>\n",
       "      <td>9f857e1a8855405dee584f3b0a2fac14c909642406de01...</td>\n",
       "      <td>[I, 'm, from, SK, but, I, 've, never, seen, th...</td>\n",
       "      <td>[i, 'm, from, sk, but, i, 've, never, seen, th...</td>\n",
       "      <td>['m, sk, 've, never, seen, life, googling, app...</td>\n",
       "      <td>['m, sk, 've, never, seen, life, googling, app...</td>\n",
       "      <td>[i, 'm, from, sk, but, i, 've, never, seen, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Okey.. nice. But how do you get on/off the bik...</td>\n",
       "      <td>2020-11-02 12:21:54</td>\n",
       "      <td>1397</td>\n",
       "      <td>Damnthatsinteresting</td>\n",
       "      <td>comment</td>\n",
       "      <td>In South Korea, the solar panels in the middle...</td>\n",
       "      <td>a21e3373140cb2d76eb06608a684a14ee1ff8cbf522943...</td>\n",
       "      <td>[Okey, .., nice, ., But, how, do, you, get, on...</td>\n",
       "      <td>[okey, .., nice, ., but, how, do, you, get, on...</td>\n",
       "      <td>[okey, .., nice, get, on/off, bike, path]</td>\n",
       "      <td>[okey, .., nice, get, on/off, bike, path]</td>\n",
       "      <td>[okey, .., nice, ., but, how, do, you, get, on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Americans: This is an amazing idea. \\nDutch Gu...</td>\n",
       "      <td>2020-11-02 12:26:04</td>\n",
       "      <td>483</td>\n",
       "      <td>Damnthatsinteresting</td>\n",
       "      <td>comment</td>\n",
       "      <td>In South Korea, the solar panels in the middle...</td>\n",
       "      <td>859600e9cf22435d4a71fa55fa336be183f38147cb013c...</td>\n",
       "      <td>[Americans, :, This, is, an, amazing, idea, .,...</td>\n",
       "      <td>[americans, :, this, is, an, amazing, idea, .,...</td>\n",
       "      <td>[americans, amazing, idea, dutch, guy, exhaust...</td>\n",
       "      <td>[americans, amazing, idea, dutch, guy, exhaust...</td>\n",
       "      <td>[american, :, this, is, an, amazing, idea, ., ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We can't have that here. America is special in...</td>\n",
       "      <td>2020-11-02 11:37:46</td>\n",
       "      <td>371</td>\n",
       "      <td>Damnthatsinteresting</td>\n",
       "      <td>comment</td>\n",
       "      <td>In South Korea, the solar panels in the middle...</td>\n",
       "      <td>490294b97dd9c899da4a4e5ba8331583b943d2e3d45e3a...</td>\n",
       "      <td>[We, ca, n't, have, that, here, ., America, is...</td>\n",
       "      <td>[we, ca, n't, have, that, here, ., america, is...</td>\n",
       "      <td>[ca, n't, america, special, every, special, ci...</td>\n",
       "      <td>[ca, n't, america, special, every, special, ci...</td>\n",
       "      <td>[we, ca, n't, have, that, here, ., america, is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>Oh bro that’s so fucking bad. Just build nucle...</td>\n",
       "      <td>2022-08-18 02:51:48</td>\n",
       "      <td>-2</td>\n",
       "      <td>Damnthatsinteresting</td>\n",
       "      <td>comment</td>\n",
       "      <td>The Belgian start-up Turbulent Hydro has inven...</td>\n",
       "      <td>dc937b59892604f5a86ac96936cd7ff09e25f18ae6b758...</td>\n",
       "      <td>[Oh, bro, that, ’, s, so, fucking, bad, ., Jus...</td>\n",
       "      <td>[oh, bro, that, ’, s, so, fucking, bad, ., jus...</td>\n",
       "      <td>[oh, bro, ’, fucking, bad, build, nuclear, lmao]</td>\n",
       "      <td>[oh, bro, , fucking, bad, build, nuclear, lmao]</td>\n",
       "      <td>[oh, bro, that, ’, s, so, fucking, bad, ., jus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>What they don't mention is that there's usuall...</td>\n",
       "      <td>2022-08-17 19:52:25</td>\n",
       "      <td>-3</td>\n",
       "      <td>Damnthatsinteresting</td>\n",
       "      <td>comment</td>\n",
       "      <td>The Belgian start-up Turbulent Hydro has inven...</td>\n",
       "      <td>063f1c9669eb28a641b7336e32fee23838f2edd8bba381...</td>\n",
       "      <td>[What, they, do, n't, mention, is, that, there...</td>\n",
       "      <td>[what, they, do, n't, mention, is, that, there...</td>\n",
       "      <td>[n't, mention, 's, usually, co2, plume, around...</td>\n",
       "      <td>[n't, mention, 's, usually, co2, plume, around...</td>\n",
       "      <td>[what, they, do, n't, mention, is, that, there...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>\"It's such a shame the inventors of this devic...</td>\n",
       "      <td>2022-08-17 19:28:36</td>\n",
       "      <td>-4</td>\n",
       "      <td>Damnthatsinteresting</td>\n",
       "      <td>comment</td>\n",
       "      <td>The Belgian start-up Turbulent Hydro has inven...</td>\n",
       "      <td>764530f6e0ce5208063f47c4ed28126e30f6bacfb80d1f...</td>\n",
       "      <td>[``, It, 's, such, a, shame, the, inventors, o...</td>\n",
       "      <td>[``, it, 's, such, a, shame, the, inventors, o...</td>\n",
       "      <td>[``, 's, shame, inventors, device, killed, two...</td>\n",
       "      <td>[``, 's, shame, inventors, device, killed, two...</td>\n",
       "      <td>[``, it, 's, such, a, shame, the, inventor, of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>“Without rivers”…..(then insert picture of riv...</td>\n",
       "      <td>2022-08-17 18:18:24</td>\n",
       "      <td>-10</td>\n",
       "      <td>Damnthatsinteresting</td>\n",
       "      <td>comment</td>\n",
       "      <td>The Belgian start-up Turbulent Hydro has inven...</td>\n",
       "      <td>7549dda1ef698b5ac4a12a4dd1a868d905981f35382690...</td>\n",
       "      <td>[“, Without, rivers, ”, …, .., (, then, insert...</td>\n",
       "      <td>[“, without, rivers, ”, …, .., (, then, insert...</td>\n",
       "      <td>[“, without, rivers, ”, …, .., insert, picture...</td>\n",
       "      <td>[, without, rivers, , , .., insert, picture, r...</td>\n",
       "      <td>[“, without, river, ”, …, .., (, then, insert,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>2022-08-17 18:41:13</td>\n",
       "      <td>-12</td>\n",
       "      <td>Damnthatsinteresting</td>\n",
       "      <td>comment</td>\n",
       "      <td>The Belgian start-up Turbulent Hydro has inven...</td>\n",
       "      <td>dc937b59892604f5a86ac96936cd7ff09e25f18ae6b758...</td>\n",
       "      <td>[[, deleted, ]]</td>\n",
       "      <td>[[, deleted, ]]</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>[[, deleted, ]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9197 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text            datetime  \\\n",
       "0    This idea is great. Unfortunately, I stay in F... 2020-11-02 12:01:56   \n",
       "1    I'm from SK but I've never seen this in my lif... 2020-11-02 13:02:00   \n",
       "2    Okey.. nice. But how do you get on/off the bik... 2020-11-02 12:21:54   \n",
       "3    Americans: This is an amazing idea. \\nDutch Gu... 2020-11-02 12:26:04   \n",
       "4    We can't have that here. America is special in... 2020-11-02 11:37:46   \n",
       "..                                                 ...                 ...   \n",
       "727  Oh bro that’s so fucking bad. Just build nucle... 2022-08-18 02:51:48   \n",
       "728  What they don't mention is that there's usuall... 2022-08-17 19:52:25   \n",
       "729  \"It's such a shame the inventors of this devic... 2022-08-17 19:28:36   \n",
       "730  “Without rivers”…..(then insert picture of riv... 2022-08-17 18:18:24   \n",
       "731                                          [deleted] 2022-08-17 18:41:13   \n",
       "\n",
       "     score             subreddit     type  \\\n",
       "0     5318  Damnthatsinteresting  comment   \n",
       "1     1798  Damnthatsinteresting  comment   \n",
       "2     1397  Damnthatsinteresting  comment   \n",
       "3      483  Damnthatsinteresting  comment   \n",
       "4      371  Damnthatsinteresting  comment   \n",
       "..     ...                   ...      ...   \n",
       "727     -2  Damnthatsinteresting  comment   \n",
       "728     -3  Damnthatsinteresting  comment   \n",
       "729     -4  Damnthatsinteresting  comment   \n",
       "730    -10  Damnthatsinteresting  comment   \n",
       "731    -12  Damnthatsinteresting  comment   \n",
       "\n",
       "                                                 title  \\\n",
       "0    In South Korea, the solar panels in the middle...   \n",
       "1    In South Korea, the solar panels in the middle...   \n",
       "2    In South Korea, the solar panels in the middle...   \n",
       "3    In South Korea, the solar panels in the middle...   \n",
       "4    In South Korea, the solar panels in the middle...   \n",
       "..                                                 ...   \n",
       "727  The Belgian start-up Turbulent Hydro has inven...   \n",
       "728  The Belgian start-up Turbulent Hydro has inven...   \n",
       "729  The Belgian start-up Turbulent Hydro has inven...   \n",
       "730  The Belgian start-up Turbulent Hydro has inven...   \n",
       "731  The Belgian start-up Turbulent Hydro has inven...   \n",
       "\n",
       "                                       redditor_hashed  \\\n",
       "0    dc937b59892604f5a86ac96936cd7ff09e25f18ae6b758...   \n",
       "1    9f857e1a8855405dee584f3b0a2fac14c909642406de01...   \n",
       "2    a21e3373140cb2d76eb06608a684a14ee1ff8cbf522943...   \n",
       "3    859600e9cf22435d4a71fa55fa336be183f38147cb013c...   \n",
       "4    490294b97dd9c899da4a4e5ba8331583b943d2e3d45e3a...   \n",
       "..                                                 ...   \n",
       "727  dc937b59892604f5a86ac96936cd7ff09e25f18ae6b758...   \n",
       "728  063f1c9669eb28a641b7336e32fee23838f2edd8bba381...   \n",
       "729  764530f6e0ce5208063f47c4ed28126e30f6bacfb80d1f...   \n",
       "730  7549dda1ef698b5ac4a12a4dd1a868d905981f35382690...   \n",
       "731  dc937b59892604f5a86ac96936cd7ff09e25f18ae6b758...   \n",
       "\n",
       "                                        text_tokenised  \\\n",
       "0    [This, idea, is, great, ., Unfortunately, ,, I...   \n",
       "1    [I, 'm, from, SK, but, I, 've, never, seen, th...   \n",
       "2    [Okey, .., nice, ., But, how, do, you, get, on...   \n",
       "3    [Americans, :, This, is, an, amazing, idea, .,...   \n",
       "4    [We, ca, n't, have, that, here, ., America, is...   \n",
       "..                                                 ...   \n",
       "727  [Oh, bro, that, ’, s, so, fucking, bad, ., Jus...   \n",
       "728  [What, they, do, n't, mention, is, that, there...   \n",
       "729  [``, It, 's, such, a, shame, the, inventors, o...   \n",
       "730  [“, Without, rivers, ”, …, .., (, then, insert...   \n",
       "731                                    [[, deleted, ]]   \n",
       "\n",
       "                                          text_lowered  \\\n",
       "0    [this, idea, is, great, ., unfortunately, ,, i...   \n",
       "1    [i, 'm, from, sk, but, i, 've, never, seen, th...   \n",
       "2    [okey, .., nice, ., but, how, do, you, get, on...   \n",
       "3    [americans, :, this, is, an, amazing, idea, .,...   \n",
       "4    [we, ca, n't, have, that, here, ., america, is...   \n",
       "..                                                 ...   \n",
       "727  [oh, bro, that, ’, s, so, fucking, bad, ., jus...   \n",
       "728  [what, they, do, n't, mention, is, that, there...   \n",
       "729  [``, it, 's, such, a, shame, the, inventors, o...   \n",
       "730  [“, without, rivers, ”, …, .., (, then, insert...   \n",
       "731                                    [[, deleted, ]]   \n",
       "\n",
       "                                text_without_stopwords  \\\n",
       "0    [idea, great, unfortunately, stay, florida, fe...   \n",
       "1    ['m, sk, 've, never, seen, life, googling, app...   \n",
       "2            [okey, .., nice, get, on/off, bike, path]   \n",
       "3    [americans, amazing, idea, dutch, guy, exhaust...   \n",
       "4    [ca, n't, america, special, every, special, ci...   \n",
       "..                                                 ...   \n",
       "727   [oh, bro, ’, fucking, bad, build, nuclear, lmao]   \n",
       "728  [n't, mention, 's, usually, co2, plume, around...   \n",
       "729  [``, 's, shame, inventors, device, killed, two...   \n",
       "730  [“, without, rivers, ”, …, .., insert, picture...   \n",
       "731                                          [deleted]   \n",
       "\n",
       "                                            text_clean  \\\n",
       "0    [idea, great, unfortunately, stay, florida, fe...   \n",
       "1    ['m, sk, 've, never, seen, life, googling, app...   \n",
       "2            [okey, .., nice, get, on/off, bike, path]   \n",
       "3    [americans, amazing, idea, dutch, guy, exhaust...   \n",
       "4    [ca, n't, america, special, every, special, ci...   \n",
       "..                                                 ...   \n",
       "727    [oh, bro, , fucking, bad, build, nuclear, lmao]   \n",
       "728  [n't, mention, 's, usually, co2, plume, around...   \n",
       "729  [``, 's, shame, inventors, device, killed, two...   \n",
       "730  [, without, rivers, , , .., insert, picture, r...   \n",
       "731                                          [deleted]   \n",
       "\n",
       "                                       text_lemmatised  \n",
       "0    [this, idea, is, great, ., unfortunately, ,, i...  \n",
       "1    [i, 'm, from, sk, but, i, 've, never, seen, th...  \n",
       "2    [okey, .., nice, ., but, how, do, you, get, on...  \n",
       "3    [american, :, this, is, an, amazing, idea, ., ...  \n",
       "4    [we, ca, n't, have, that, here, ., america, is...  \n",
       "..                                                 ...  \n",
       "727  [oh, bro, that, ’, s, so, fucking, bad, ., jus...  \n",
       "728  [what, they, do, n't, mention, is, that, there...  \n",
       "729  [``, it, 's, such, a, shame, the, inventor, of...  \n",
       "730  [“, without, river, ”, …, .., (, then, insert,...  \n",
       "731                                    [[, deleted, ]]  \n",
       "\n",
       "[9197 rows x 12 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print the DataFrame to see if everyting is good\n",
    "\n",
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if wanted in Excel run this cell\n",
    "\n",
    "df_combined.to_excel('Summative_1_DataFrame_21008600514.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if wanted in CSV run this cell \n",
    "\n",
    "df_combined.to_csv('Summative_1_DataFrame_21008600514.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access the Data, \n",
    "Enter your reddit authentication details in the second cell, then run the whole programme and either of the cells above to get the code in the desired format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#500 word description below "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "For this project the long term plan is to gain an insight into the attitudes surrounding different renewable alternatives to non-renewable energy sources. For this I will be using reddit to find posts mentioning different alternative energy sources namely Solar, Wind, Nuclear Fusion, Nuclear fission, Hydropower and analyse the comments around each one to determine what the sentiment around the different sources are currently. \n",
    "To do collect the data, I first searched through reddit to find the posts that have the most upvotes and most comments. Then I selected the posts that actually spoke about the application or innovations in the alternative energy source I was looking at which I added to a list. Unfortunately not all the different categories I would like to analyse have the same amount of posts or comments which could affect the validity of the findings\n",
    "I then wrote the code to be able to extract the data from reddit. I started by importing all the libraries required to obtain the data and process it for NLP. Key Libraries used in this project are PRAW, Pandas, NLTK. After uploading my authentication information, I ran a quick test code to see if everything was working\n",
    "I then started on working on  a function that was previously given as an example to only extract information from a post on reddit. I used that as base code to build off. Based off the basic DataFrame the base code provides, I firstly added a line of code which instantly anonymised the redditors usernames using sha256 encoding. I then tokenised the data and added it to a separate column so that I could track the work the code was doing to make sure it was doing the right things. I then set all the items to lower case. I did this first because the list of stop words are all in lower case. If I had not set all the words to lower case, there would still be stop words in the data set. I then proceeded to data points of anything value that isn’t a word by encoding all the values into ASCII and ignoring whatever can’t be encoded and decoding it again to remove all unwanted characters. Then I simply lemmatised the remaining data. All these different steps were all added to separate columns in the DataFrame so that the progression the cleaning process can be seen. The Data in the ‘text_lemmatised’ column is all ready for NLP analysis. The function set up so that any link put into it is ready for NLP analysis.\n",
    "I then created multiple for loops which iterated through the list of links provided to process the data and append it to dataframe called df_combined which contains all the comments extracted through the API. For further NLP analysis I intend on expanding the number of posts and comments on each topic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
